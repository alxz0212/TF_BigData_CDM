# Paso 4: Reflexi√≥n IA - "3 Momentos Clave"

**Alumno:** Daniel Alexis Mendoza Corne  
**Fecha:** Febrero 2026

---

## Bloque A: Infraestructura (Docker)

### 1. Arranque

**¬øQu√© fue lo primero que le pediste a la IA?**  
Le ped√≠ generar un archivo `docker-compose.yml` que incluyera servicios para Spark (Master y Worker), PostgreSQL como base de datos y JupyterLab como entorno de desarrollo interactivo, asegurando la conexi√≥n entre ellos.

### 2. Error

**¬øQu√© fall√≥ y c√≥mo lo resolviste?**  
Al intentar verificar los servicios, intent√© acceder v√≠a navegador a los puertos `7077` (Spark Master) y `5432` (Postgres) obteniendo una p√°gina de error ("Empty Response").

- **Resoluci√≥n:** Aprend√≠ que esos son puertos de comunicaci√≥n interna (TCP) para los servicios, no interfaces web HTTP. Me dirig√≠ a los puertos correctos visuales: `8080` (Spark UI) y `8888` (JupyterLab).

**Otro Error Detectado: PySpark Module**
- **Fallo:** `ModuleNotFoundError: No module named 'pyspark'` al correr scripts internos.
- **Causa:** La imagen base contiene Spark pero no el paquete pip accesible por defecto en scripts externos.
- **Resoluci√≥n:** Se a√±adi√≥ expl√≠citamente `pyspark==3.5.0` en `requirements.txt` para hacer match con la versi√≥n binaria del contenedor.

### 3. Aprendizaje

**¬øQu√© aprendiste que NO sab√≠as antes?**  
La diferencia cr√≠tica entre los puertos expuestos para clientes (Navegador) y los puertos de servicio interno en Docker. Tambi√©n c√≥mo persistir datos usando `volumes` para no perder mis notebooks al reiniciar el contenedor.

**Otro Error Detectado: Spark Worker Offline**
- **Fallo:** En la interfaz `localhost:8080`, aparec√≠a "Alive Workers: 0" aunque el contenedor exist√≠a.
- **Causa:** Al reconstruir y levantar solo el servicio `jupyter-lab`, docker-compose no necesariamente reinicia o mantiene activos los contenedores dependientes si no se especifican.
- **Resoluci√≥n:** Ejecutar `docker-compose up -d` (sin especificar servicio) y verificar con `docker ps` asegur√≥ que tanto Master como Worker estuvieran activos.
- **Aprendizaje:** La "Arquitectura Distribuida" requiere validaci√≥n expl√≠cita de que todos los nodos est√°n vivos, no basta con que el c√≥digo corra (que puede estar en modo local).

### üí¨ Prompt Clave (Bloque A)

```text
"enrtonces ya esta actualizado 02_INFRAESTRUCTURA..md y docker-compose.yml para verificar?"
```

---

## Bloque B: Pipeline ETL (Spark)

### 1. Arranque

**¬øQu√© fue lo primero que le pediste a la IA?**  
C√≥mo estructurar un script `pipeline.py` que leyera el dataset QoG, filtrara espec√≠ficamente los 5 pa√≠ses seleccionados de mi zona de estudio ("Gran Juego") y creara una variable derivada para agruparlos por subregi√≥n.

### 2. Error

**¬øQu√© fall√≥ y c√≥mo lo resolviste?**  
Tuve dificultades iniciales con el control de versiones al intentar subir el proyecto.
**Error:** `fatal: not a git repository`.

- **Resoluci√≥n:** Inicialic√© correctamente el repositorio con `git init`, configur√© el `.gitignore` protegiendo la carpeta `data/` y a√±ad√≠ el remoto de GitHub correctamente antes de hacer el push.

### 3. Aprendizaje

**¬øQu√© aprendiste que NO sab√≠as antes?**  
C√≥mo utilizar `pyspark.sql.functions.when` para crear columnas condicionales complejas (variable `subregion`) de manera eficiente en un DataFrame distribuido, evitando bucles `for` de Python que son ineficientes en Big Data.

### üí¨ Prompt Clave (Bloque B)

```text
"sube todo a mi github https://github.com/alxz0212/Avance_TF_CDM"
```

---

## Bloque C: An√°lisis de Datos (Machine Learning)

### 1. Arranque

**¬øQu√© fue lo primero que le pediste a la IA?**  
Solicit√© asesoramiento para elegir el mejor modelo de Machine Learning (entre KNN, SVM y Random Forest) dado mi objetivo de explicar la influencia de factores pol√≠ticos en la econom√≠a.

### 2. Error

**¬øQu√© fall√≥ y c√≥mo lo resolviste?**  
Al intentar generar los gr√°ficos autom√°ticamente ejecutando el notebook desde la terminal con `nbconvert`.
**Error:**

```text
TypeError: 'JavaPackage' object is not callable
...
spark = SparkSession.builder...
```

- **Resoluci√≥n:** El entorno de ejecuci√≥n autom√°tica ten√≠a conflictos con la sesi√≥n de Spark existente. Migr√© la l√≥gica a un script python dedicado (`src/analysis.py`) ejecutado con `spark-submit`, lo cual demostr√≥ ser mucho m√°s robusto para tareas de producci√≥n.

### 3. Aprendizaje

**¬øQu√© aprendiste que NO sab√≠as antes?**  
Que `Random Forest` no solo predice, sino que ofrece la m√©trica `featureImportances` que sirve para explicar causalidad ("Explicabilidad del modelo"), haci√©ndolo superior a KNN para mi pregunta de investigaci√≥n. Tambi√©n aprend√≠ a automatizar la generaci√≥n de gr√°ficos sin abrir Jupyter manualmente.

### üí¨ Prompt Clave (Bloque C)

```text
"Antes de empezar quisiera aplicar dentro del analisis uno de estos modelos KNN , SVM o Ramdom forest cual crees que seria mejor teniendo en cuenta la data que tengo"
```
