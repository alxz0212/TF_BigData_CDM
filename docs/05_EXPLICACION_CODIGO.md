# üìòPaso 5: Documentaci√≥n T√©cnica del C√≥digo Fuente

**Proyecto:** Big Data & Geopol√≠tica ("El Gran Juego")
**Alumno:** Daniel Alexis Mendoza Corne
**Fecha:** Febrero 2026

---

## 1. ¬øPor qu√© la carpeta se llama `src`?

`src` es la abreviatura est√°ndar en ingenier√≠a de software para **"Source"** (C√≥digo Fuente).

En proyectos profesionales, es fundamental mantener separado el c√≥digo l√≥gico de otros elementos. Esta estructura garantiza:

- **Orden:** El c√≥digo no se mezcla con la documentaci√≥n (`.md`), la configuraci√≥n (`docker/`) o los datos (`data/`).
- **Seguridad:** Facilita la configuraci√≥n de permisos; por ejemplo, el servidor de producci√≥n solo necesita acceso de lectura a `src`, pero de escritura a `data`.
- **Escalabilidad:** Si el proyecto crece, todo el c√≥digo l√≥gica reside en un √∫nico punto de verdad.

---

## 2. Cat√°logo de Scripts

A continuaci√≥n, se detalla la funci√≥n t√©cnica y de negocio de cada m√≥dulo desarrollado.

### üõ†Ô∏è 1. Infraestructura y Preparaci√≥n

#### `download_data.py`

- **Funci√≥n:** Automatizaci√≥n de Ingesta.
- **Qu√© hace:** Se conecta al repositorio de la Universidad de Gotemburgo, descarga el dataset `.csv` de 68MB y lo coloca en la ruta `data/raw/`.
- **Por qu√© es importante:** Elimina la dependencia de descargas manuales, haciendo que el proyecto sea reproducible en cualquier m√°quina con un solo comando.

#### `verify_spark.py`

- **Funci√≥n:** Test de Integridad (Smoke Test).
- **Qu√© hace:** Intenta iniciar una sesi√≥n de Spark y crear un DataFrame peque√±o en memoria.
- **Por qu√© es importante:** Es el primer script que ejecutamos para validar que el contenedor de Docker y el cluster de Spark est√°n comunic√°ndose correctamente antes de lanzar procesos pesados.

---

### ‚öôÔ∏è 2. Procesamiento de Datos (ETL)

#### `pipeline.py`

- **Funci√≥n:** ETL (Extract, Transform, Load).
- **Tecnolog√≠a:** Apache Spark (PySpark SQL).
- **Flujo de Trabajo:**
  1.  **Extract:** Lee el CSV crudo.
  2.  **Transform:**
      - Filtra los 5 pa√≠ses del "Gran Juego" (Afganist√°n, Mongolia, C√°ucaso).
      - Crea la variable derivada `subregion`.
      - Castea tipos de datos (Strings a Doubles) para asegurar precisi√≥n matem√°tica.
  3.  **Load:** Guarda el resultado limpio en formato **Parquet**.
- **Detalle Pro:** Usamos `.parquet` en lugar de `.csv` porque es un formato columnar comprimido que es mucho m√°s r√°pido para leer en an√°lisis posteriores de Big Data.

#### `ingest_data.py` (M√≥dulo Legado)

- **Funci√≥n:** Conector a Base de Datos Relacional.
- **Qu√© hace:** Estaba dise√±ado para cargar los datos en PostgreSQL.
- **Estado:** Se mantiene como respaldo. Para el an√°lisis principal optamos por el flujo Spark-Parquet por ser m√°s nativo del ecosistema de Big Data que el almacenamiento SQL tradicional.

---

### üß† 3. An√°lisis Avanzado y Resultados

#### `analysis.py`

- **Funci√≥n:** Motor de Machine Learning.
- **Tecnolog√≠a:** Spark MLlib.
- **Qu√© hace:**
  - Carga los datos procesados (Parquet).
  - **Matriz de Correlaci√≥n:** Calcula c√≥mo se relacionan las variables (ej. Gasto Militar vs PIB).
  - **Random Forest:** Entrena un modelo de Inteligencia Artificial compuesto por 100 √°rboles de decisi√≥n para predecir el desarrollo econ√≥mico.
  - **Feature Importance:** Extrae qu√© variables tuvieron m√°s peso en la decisi√≥n del modelo.
- **Salida:** Genera autom√°ticamente los gr√°ficos est√°ticos `.png` en la carpeta `notebooks/`.

#### `econometric_analysis.py`

- **Funci√≥n:** An√°lisis Econom√©trico Riguroso.
- **Tecnolog√≠a:** Librer√≠a `linearmodels` (Python).
- **Qu√© hace:**
  - Ejecuta modelos de regresi√≥n para datos de panel: **Efectos Fijos (Fixed Effects)** y **Efectos Aleatorios (Random Effects)**.
  - Implementa el **Test de Hausman** para determinar cu√°l de los dos modelos es estad√≠sticamente m√°s adecuado (causalidad vs correlaci√≥n).
  - Genera un reporte detallado en `notebooks/hausman_results.txt`.
- **Valor agregado:** Complementa la "caja negra" del Machine Learning (Random Forest) con inferencia estad√≠stica cl√°sica, validando si las caracter√≠sticas √∫nicas de cada pa√≠s sesgan los resultados.

---

### üöÄ 4. Interfaz de Usuario (Frontend)

#### `src/app_streamlit.py` y `src/app_streamlit_pro.py`

Son el Frontend de la aplicaci√≥n.

- **Tecnolog√≠a:** Streamlit.
- **Funciones:**
  - Cargar el Parquet procesado.
  - Generar gr√°ficos interactivos con Plotly.
  - **Pro Version:** Incluye globo 3D, radar charts y est√©tica "Dark Mode".
  - Sirve una interfaz web en el puerto `8501`.
  - Permite al usuario explorar los datos: filtrar por a√±o, ver tendencias temporales interactivas y simular predicciones.
  - Es la "cara" del proyecto, transformando el c√≥digo t√©cnico en un producto visual consumible por un usuario final.

---

## 3. Diagrama de Flujo de Datos

```mermaid
graph TD
    %% Estilos
    classDef source fill:#f9f,stroke:#333,stroke-width:2px;
    classDef script fill:#bbf,stroke:#333,stroke-width:2px,color:black;
    classDef data fill:#dfd,stroke:#333,stroke-width:2px,color:black;
    classDef output fill:#fd9,stroke:#333,stroke-width:2px,color:black,stroke-dasharray: 5 5;

    subgraph INGESTA ["üì° Ingesta de Datos"]
        A["‚òÅÔ∏è Internet / Repo QoG"]:::source
        Script1{{"üêç download_data.py"}}:::script
    end

    subgraph PROCESAMIENTO ["‚öôÔ∏è Procesamiento & An√°lisis"]
        Script2{{"‚ö° pipeline.py"}}:::script
        Script3{{"üß† analysis.py"}}:::script
        Script5{{"üìâ econometric_analysis.py"}}:::script
    end

    subgraph ALMACENAMIENTO ["üíæ Almacenamiento"]
        B[("üìÑ Raw CSV")]:::data
        C[("üì¶ Clean Parquet")]:::data
    end

    subgraph VISUALIZACION ["üìä Consumo & UI"]
        Script4{{"üöÄ app_streamlit_pro.py"}}:::script
        D["üìà Gr√°ficos Est√°ticos .png"]:::output
        E["üñ•Ô∏è Dashboard 3D Interactivo"]:::output
        F["üìÑ Reporte Hausman .txt"]:::output
    end

    %% Relaciones
    A --> Script1
    Script1 --> B
    B --> Script2
    Script2 --> C
    C --> Script3
    C --> Script4
    C --> Script5
    Script3 --> D
    Script4 --> E
    Script5 --> F
```

> [!NOTE]
> **Conclusi√≥n del Flujo de Datos:**  
> Como se observa en el diagrama, el proyecto sigue una arquitectura lineal de Big Data moderna:
>
> 1.  **Ingesta:** Los datos se capturan autom√°ticamente de internet (`download_data.py`).
> 2.  **Procesamiento:** Se limpian y estructuran en Spark (`pipeline.py`), guard√°ndose en formato eficiente **Parquet**.
> 3.  **Consumo:** A partir del dato limpio, se derivan tres productos finales: An√°lisis ML (`analysis.py`), Validaci√≥n Estad√≠stica (`econometric_analysis.py`) y Visualizaci√≥n Interactiva (`app_streamlit_pro.py`).
>
> Esta estructura modular asegura que si cambiamos la fuente de datos, solo tocamos el script de _Ingesta_, sin romper el Dashboard final.

---

## 4. DevOps y Documentaci√≥n üìö

Para desplegar este sitio web, utilizamos dos archivos clave que a menudo se confunden pero tienen prop√≥sitos muy distintos:

### `mkdocs.yml` (El Cerebro üß†)

**Ubicaci√≥n:** Ra√≠z del proyecto.
**Funci√≥n:** Configuraci√≥n del Sitio Web.
**Qu√© hace:**

- Define el t√≠tulo del sitio, el autor y el tema visual ("Material").
- Estructura el men√∫ de navegaci√≥n lateral.
- Activa plugins y extensiones (como Mermaid para los gr√°ficos).
- **Es el archivo que t√∫ editas** cuando quieres cambiar el contenido, el orden de las p√°ginas o el color del sitio.

### `.github/workflows/deploy_docs.yml` (El Obrero üë∑)

**Ubicaci√≥n:** `.github/workflows/` (antes llamado `mkdocs.yml`).
**Funci√≥n:** Automatizaci√≥n del Despliegue (CI/CD).
**Qu√© hace:**

- Es un script de instrucciones para los servidores de GitHub (GitHub Actions).
- Cada vez que haces un cambio (`git push`), este archivo le dice a GitHub:
  1. "Instala Python y MkDocs".
  2. "Instala los plugins necesarios (Material, Mermaid)".
  3. "Construye la p√°gina web est√°tica".
  4. "Publicala en internet (GitHub Pages)".
- **No necesitas editarlo casi nunca**, salvo que cambies la forma de desplegar el sitio.
