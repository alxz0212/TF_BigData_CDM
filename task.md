# Tareas del Proyecto: TF Pipeline BigData con Infraestructura Docker

- [x] Análisis de Requisitos <!-- id: 0 -->
  - [x] Crear estructura de carpetas <!-- id: 1 -->
  - [x] Confirmar etapas del proyecto (basado en la imagen) <!-- id: 2 -->
- [x] Etapa 1: Infraestructura (Docker) <!-- id: 3 -->
  - [x] Definir `Dockerfile` para Spark y TensorFlow <!-- id: 4 -->
  - [x] Definir `docker-compose.yml` (Jupyter, Spark, etc.) <!-- id: 5 -->
  - [x] Verificar levantamiento de servicios <!-- id: 6 -->
- [x] Etapa 2: Ingesta y Procesamiento de Datos <!-- id: 7 -->
  - [x] Descarga/Carga del Dataset <!-- id: 8 -->
  - [x] Limpieza y transformación con PySpark <!-- id: 9 -->
- [x] Etapa 3: Análisis y Modelado (Random Forest en Spark) <!-- id: 10 -->
  - [x] Preparación de datos (VectorAssembler) <!-- id: 11 -->
  - [x] Entrenamiento del modelo (Random Forest) <!-- id: 12 -->
  - [x] Evaluación (Importancia de Variables) <!-- id: 13 -->
- [x] Etapa 4: Documentación y Entrega <!-- id: 14 -->
  - [x] Redactar `README.md` final <!-- id: 15 -->
  - [x] Resumen de planes y justificación <!-- id: 16 -->
